{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55262d1f",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa75b3ad",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10439ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User;Response',\n",
       " 'Apa itu rendang?;Rendang adalah masakan daging berasal dari Minangkabau yang dimasak lama dengan santan dan rempah hingga kering.',\n",
       " 'Apa bahan utama membuat rendang sapi?;Bahan utamanya adalah daging sapi, santan kelapa tua, dan campuran bumbu halus serta rempah daun.',\n",
       " 'Bagian daging sapi apa yang terbaik untuk rendang?;Paha belakang (knuckle) adalah yang terbaik karena teksturnya padat dan tidak mudah hancur.',\n",
       " 'Mengapa rendang dimasak sangat lama?;Tujuannya agar santan terkaramelisasi menjadi minyak dan bumbu meresap sempurna ke dalam serat daging.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"dataset.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = file.read().splitlines()\n",
    "\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb8f655",
   "metadata": {},
   "source": [
    "Convert ke format JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85eab725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def txt_qa_to_jsonl(input_path: str, output_path: str, encoding: str = \"utf-8\"):\n",
    "\n",
    "    with open(input_path, \"r\", encoding=encoding) as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    lines = lines[1:]\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        if \";\" not in line:\n",
    "            continue\n",
    "\n",
    "        instruction, response = line.split(\";\", 1)\n",
    "\n",
    "        instruction = instruction.strip()\n",
    "        response = response.strip()\n",
    "\n",
    "        if not instruction or not response:\n",
    "            continue\n",
    "\n",
    "        text = (\n",
    "            \"### Instruction:\\n\"\n",
    "            f\"{instruction}\\n\\n\"\n",
    "            \"### Response:\\n\"\n",
    "            f\"{response}\"\n",
    "        )\n",
    "\n",
    "        samples.append({\"text\": text})\n",
    "\n",
    "    with open(output_path, \"w\", encoding=encoding) as f:\n",
    "        for sample in samples:\n",
    "            f.write(json.dumps(sample, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63188d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_qa_to_jsonl(\"dataset_850.txt\", \"dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e119e",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0979874f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\THINKPAD\\anaconda3\\envs\\rendang_gpt\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt2-small-indonesian\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt2-small-indonesian\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e9d5f1",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae72826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 869 examples [00:00, 76097.67 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 869/869 [00:00<00:00, 2947.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"dataset.jsonl\", split=\"train\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70147b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['### Instruction:\\nApa itu rendang?\\n\\n### Response:\\nRendang adalah masakan daging berasal dari Minangkabau yang dimasak lama dengan santan dan rempah hingga kering.',\n",
       "  '### Instruction:\\nApa bahan utama membuat rendang sapi?\\n\\n### Response:\\nBahan utamanya adalah daging sapi, santan kelapa tua, dan campuran bumbu halus serta rempah daun.',\n",
       "  '### Instruction:\\nBagian daging sapi apa yang terbaik untuk rendang?\\n\\n### Response:\\nPaha belakang (knuckle) adalah yang terbaik karena teksturnya padat dan tidak mudah hancur.',\n",
       "  '### Instruction:\\nMengapa rendang dimasak sangat lama?\\n\\n### Response:\\nTujuannya agar santan terkaramelisasi menjadi minyak dan bumbu meresap sempurna ke dalam serat daging.',\n",
       "  '### Instruction:\\nApa perbedaan gulai, kalio, dan rendang?\\n\\n### Response:\\nGulai masih berkuah encer, kalio berkuah kental berminyak, dan rendang sudah kering serta berwarna gelap.']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be476b6",
   "metadata": {},
   "source": [
    "# Setup LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfaba134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\THINKPAD\\anaconda3\\envs\\rendang_gpt\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\"] \n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e787b4f6",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7d9be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mulai training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='550' max='550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [550/550 4:43:11, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.162600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.748400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.418200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.976500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.548700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.493500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.277500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.226300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.068600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.053500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.972600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.972900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.991200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.955400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.885700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.931600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.900600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.890200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.893600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.921200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.843800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.890200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.871300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.891400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.764900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.837500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.876100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.845200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.811700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.817200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.866500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.802400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.790300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.832300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.816600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.862200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.820400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.760300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.806800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.738200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.765400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.843700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.793800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.796600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.782700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.754800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selesai! Model tersimpan di folder 'gpt2-rendang-final-850'\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-rendang-850\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    use_cpu=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Mulai training...\")\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./gpt2-rendang-final-850\")\n",
    "print(\"Selesai! Model tersimpan di folder 'gpt2-rendang-final-850'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff461321",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d129c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sedang memuat model hasil training...\n",
      "âœ… Model berhasil dimuat dengan adapter LoRA\n",
      "Sedang menghitung Perplexity dan Loss...\n",
      "\n",
      "==================================================\n",
      "ðŸ“Š HASIL EVALUASI MODEL\n",
      "==================================================\n",
      "âœ… Perplexity Score: 76.4348\n",
      "âœ… Average Loss: 2.8407\n",
      "==================================================\n",
      "Catatan:\n",
      "- Semakin rendah Perplexity (mendekati 1), semakin baik model\n",
      "- Semakin rendah Loss, semakin baik akurasi prediksi\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model_path = \"flax-community/gpt2-small-indonesian\"\n",
    "adapter_path = \"./gpt2-rendang-final\"\n",
    "\n",
    "print(\"Sedang memuat model hasil training...\")\n",
    "model_eval = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
    "model_eval = PeftModel.from_pretrained(model_eval, adapter_path)\n",
    "\n",
    "model_eval.to(device)\n",
    "model_eval.eval()\n",
    "\n",
    "tokenizer_eval = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer_eval.pad_token = tokenizer_eval.eos_token\n",
    "\n",
    "print(\"âœ… Model berhasil dimuat dengan adapter LoRA\")\n",
    "\n",
    "def calculate_perplexity_and_loss(text_list, model, tokenizer):\n",
    "    encodings = tokenizer(\"\\n\\n\".join(text_list), return_tensors=\"pt\")\n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "    nlls = []\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "    prev_end_loc = 0\n",
    "    \n",
    "    for begin_loc in range(0, seq_len, stride):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - begin_loc  # Perbaikan: hitung dari begin_loc\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss * trg_len\n",
    "            \n",
    "            # Simpan loss\n",
    "            total_loss += outputs.loss.item()\n",
    "            total_batches += 1\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "    avg_loss = total_loss / total_batches if total_batches > 0 else 0\n",
    "    \n",
    "    return ppl.item(), avg_loss\n",
    "\n",
    "texts_to_eval = dataset[\"text\"][:50]\n",
    "\n",
    "print(\"Sedang menghitung Perplexity dan Loss...\")\n",
    "ppl_score, loss_score = calculate_perplexity_and_loss(texts_to_eval, model_eval, tokenizer_eval)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"ðŸ“Š HASIL EVALUASI MODEL\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"âœ… Perplexity Score: {ppl_score:.4f}\")\n",
    "print(f\"âœ… Average Loss: {loss_score:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(\"Catatan:\")\n",
    "print(\"- Semakin rendah Perplexity (mendekati 1), semakin baik model\")\n",
    "print(\"- Semakin rendah Loss, semakin baik akurasi prediksi\")\n",
    "print(f\"{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "994b20a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\THINKPAD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sedang memuat model hasil training...\n",
      "âœ… Model berhasil dimuat dengan adapter LoRA\n",
      "Sedang menghitung BLEU Score...\n",
      "\n",
      "==================================================\n",
      "ðŸ“Š HASIL EVALUASI MODEL\n",
      "==================================================\n",
      "âœ… BLEU Score: 0.0570\n",
      "==================================================\n",
      "Catatan:\n",
      "- BLEU Score berkisar 0-1 (semakin tinggi semakin baik)\n",
      "- BLEU Score mengukur kesamaan output dengan referensi\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data jika belum ada\n",
    "nltk.download('punkt')\n",
    "\n",
    "base_model_path = \"flax-community/gpt2-small-indonesian\"\n",
    "adapter_path = \"./gpt2-rendang-final\"\n",
    "\n",
    "print(\"Sedang memuat model hasil training...\")\n",
    "model_eval = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
    "model_eval = PeftModel.from_pretrained(model_eval, adapter_path)\n",
    "\n",
    "model_eval.to(device)\n",
    "model_eval.eval()\n",
    "\n",
    "tokenizer_eval = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer_eval.pad_token = tokenizer_eval.eos_token\n",
    "\n",
    "print(\"âœ… Model berhasil dimuat dengan adapter LoRA\")\n",
    "\n",
    "def calculate_bleu_score(text_list, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Hitung BLEU score dengan membandingkan teks asli sebagai referensi\n",
    "    dan teks yang di-generate sebagai candidate\n",
    "    \"\"\"\n",
    "    bleu_scores = []\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    \n",
    "    for text in text_list:\n",
    "        # Split teks menjadi instruction dan response\n",
    "        if \"### Response:\" in text:\n",
    "            instruction_part = text.split(\"### Instruction:\\n\")[1].split(\"\\n\\n### Response:\")[0]\n",
    "            reference_response = text.split(\"### Response:\\n\")[1]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Generate response dari instruction\n",
    "        prompt = f\"### Instruction:\\n{instruction_part}\\n\\n### Response:\\n\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=False,  # Gunakan greedy decoding untuk konsistensi\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Ambil bagian response\n",
    "        if \"### Response:\" in generated_text:\n",
    "            generated_response = generated_text.split(\"### Response:\\n\")[1]\n",
    "        else:\n",
    "            generated_response = generated_text\n",
    "        \n",
    "        # Tokenize untuk BLEU calculation\n",
    "        reference_tokens = reference_response.lower().split()\n",
    "        generated_tokens = generated_response.lower().split()\n",
    "        \n",
    "        # Hitung BLEU score (menggunakan unigram dan bigram)\n",
    "        bleu = sentence_bleu(\n",
    "            [reference_tokens],\n",
    "            generated_tokens,\n",
    "            weights=(0.5, 0.5),  # unigram 50%, bigram 50%\n",
    "            smoothing_function=smoothing_function\n",
    "        )\n",
    "        bleu_scores.append(bleu)\n",
    "    \n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "    return avg_bleu\n",
    "\n",
    "texts_to_eval = dataset[\"text\"][:50]\n",
    "\n",
    "print(\"Sedang menghitung BLEU Score...\")\n",
    "bleu_score = calculate_bleu_score(texts_to_eval, model_eval, tokenizer_eval)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"ðŸ“Š HASIL EVALUASI MODEL\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"âœ… BLEU Score: {bleu_score:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(\"Catatan:\")\n",
    "print(\"- BLEU Score berkisar 0-1 (semakin tinggi semakin baik)\")\n",
    "print(\"- BLEU Score mengukur kesamaan output dengan referensi\")\n",
    "print(f\"{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0317c178",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7e7daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat model untuk tes...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1. Setup Model (Sama seperti evaluasi)\n",
    "base_model_path = \"flax-community/gpt2-small-indonesian\"\n",
    "adapter_path = \"./gpt2-rendang-final\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Memuat model untuk tes...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_resep(pertanyaan):\n",
    "    # Format prompt HARUS SAMA PERSIS dengan saat training\n",
    "    prompt = f\"### Instruction:\\n{pertanyaan}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate jawaban\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,      # Batasi panjang jawaban biar gak ngelantur\n",
    "            do_sample=True,          # Supaya jawaban bervariasi\n",
    "            temperature=0.4,         # Kreativitas (0.1 kaku, 1.0 liar)\n",
    "            top_k=50,                # Ambil 50 kata terbaik\n",
    "            top_p=0.95,              # Ambil probabilitas kumulatif 95%\n",
    "            repetition_penalty=1.2,  # Cegah pengulangan kata\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode hasil (ubah angka jadi teks)\n",
    "    hasil_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Ambil bagian Response saja\n",
    "    if \"### Response:\" in hasil_text:\n",
    "        jawaban = hasil_text.split(\"### Response:\\n\")[1]\n",
    "    else:\n",
    "        jawaban = hasil_text\n",
    "        \n",
    "    return jawaban.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aa27af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Pertanyaan: Bagaimana cara membuat rendang?\n",
      "Model sedang berpikir...\n",
      "------------------------------\n",
      "Jawaban Model:\n",
      "Cara membuat rendang adalah dengan menggunakan daging sapi yang sudah dipotong-potong dan kemudian diulek hingga halus. Setelah itu, baru kemudian dicampur dengan bumbu rempah seperti lengkuas dan daun salam.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 30)\n",
    "pertanyaan_kamu = \"Bagaimana cara membuat rendang?\"\n",
    "print(f\"Pertanyaan: {pertanyaan_kamu}\")\n",
    "print(\"Model sedang berpikir...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "jawaban_model = generate_resep(pertanyaan_kamu)\n",
    "print(f\"Jawaban Model:\\n{jawaban_model}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0dc2be",
   "metadata": {},
   "source": [
    "# Analisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5dbf29",
   "metadata": {},
   "source": [
    "### Keamanan \n",
    "Sangat aman karena semua proses pengumpulan data, training, dan inference dilakukan di lokal dan tidak dikirim ke API publik (third party)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d7583a",
   "metadata": {},
   "source": [
    "### Privasi Pengguna\n",
    "Perlindungan privasi pengguna dari sisi aplikasi sudah memadai karena menggunakan sistem sesi sementara, di mana riwayat percakapan tidak disimpan permanen dan akan langsung hilang begitu pengguna menutup browser. Namun, terdapat catatan kritis pada sisi model (Model 2.0) yang terbukti mengalami Data Leakage dengan memunculkan teks \"Baca juga artikel...\", menandakan bahwa model dapat memuntahkan data mentah dari masa lalunya. Ini menjadi peringatan keras bahwa dataset pelatihan tidak boleh mengandung informasi identitas pribadi (PII) karena berisiko muncul kembali dalam jawaban model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a3a152",
   "metadata": {},
   "source": [
    "### Etika AI\n",
    "Dari segi etika, proyek ini menghadapi tantangan serius terkait akurasi dan keselamatan informasi, terlihat jelas dari halusinasi Model 2.0 yang memberikan instruksi menyesatkan (memasak rendang dikukus dan diberi keju) serta Model 1.0 yang mengalami pengulangan teks tak terkendali (spam). Kegagalan memberikan informasi yang benar ini berpotensi merugikan pengguna (pemborosan bahan makanan), sehingga secara etis saya wajib menyertakan disclaimer yang menyatakan bahwa AI masih dalam tahap eksperimen dan rentan terhadap bias pengetahuan umum yang menutupi pengetahuan spesifik lokal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d697da1",
   "metadata": {},
   "source": [
    "# Recommendation action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6dc01",
   "metadata": {},
   "source": [
    "- Perkaya Dataset (Wajib): Tambah data latih jadi resep spesifik rendang untuk menghilangkan halusinasi (info ngawur).\n",
    "\n",
    "- Tuning Parameter: Cegah pengulangan teks (spam) dengan menaikkan repetition_penalty ke 1.5 dan turunkan temperature ke 0.3.\n",
    "\n",
    "- Safety UI: Tambahkan Disclaimer statis pada aplikasi bahwa resep yang dihasilkan AI memerlukan verifikasi manusia sebelum dimasak.\n",
    "\n",
    "- Upgrade Model (Opsional): Jika spesifikasi memungkinkan, migrasi ke model yang lebih modern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rendang_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
