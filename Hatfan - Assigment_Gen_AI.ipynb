{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55262d1f",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa75b3ad",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d10439ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User;Response',\n",
       " 'Apa itu rendang?;Rendang adalah masakan daging berasal dari Minangkabau yang dimasak lama dengan santan dan rempah hingga kering.',\n",
       " 'Apa bahan utama membuat rendang sapi?;Bahan utamanya adalah daging sapi, santan kelapa tua, dan campuran bumbu halus serta rempah daun.',\n",
       " 'Bagian daging sapi apa yang terbaik untuk rendang?;Paha belakang (knuckle) adalah yang terbaik karena teksturnya padat dan tidak mudah hancur.',\n",
       " 'Mengapa rendang dimasak sangat lama?;Tujuannya agar santan terkaramelisasi menjadi minyak dan bumbu meresap sempurna ke dalam serat daging.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"dataset.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = file.read().splitlines()\n",
    "\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb8f655",
   "metadata": {},
   "source": [
    "Convert ke format JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85eab725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def txt_qa_to_jsonl(input_path: str, output_path: str, encoding: str = \"utf-8\"):\n",
    "\n",
    "    with open(input_path, \"r\", encoding=encoding) as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    lines = lines[1:]\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        if \";\" not in line:\n",
    "            continue\n",
    "\n",
    "        instruction, response = line.split(\";\", 1)\n",
    "\n",
    "        instruction = instruction.strip()\n",
    "        response = response.strip()\n",
    "\n",
    "        if not instruction or not response:\n",
    "            continue\n",
    "\n",
    "        text = (\n",
    "            \"### Instruction:\\n\"\n",
    "            f\"{instruction}\\n\\n\"\n",
    "            \"### Response:\\n\"\n",
    "            f\"{response}\"\n",
    "        )\n",
    "\n",
    "        samples.append({\"text\": text})\n",
    "\n",
    "    with open(output_path, \"w\", encoding=encoding) as f:\n",
    "        for sample in samples:\n",
    "            f.write(json.dumps(sample, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63188d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_qa_to_jsonl(\"dataset.txt\", \"dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e119e",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0979874f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\THINKPAD\\anaconda3\\envs\\rendang_gpt\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\THINKPAD\\.cache\\huggingface\\hub\\models--flax-community--gpt2-small-indonesian. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt2-small-indonesian\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt2-small-indonesian\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e9d5f1",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ae72826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 349/349 [00:00<00:00, 1417.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"dataset.jsonl\", split=\"train\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f70147b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['### Instruction:\\nApa itu rendang?\\n\\n### Response:\\nRendang adalah masakan daging berasal dari Minangkabau yang dimasak lama dengan santan dan rempah hingga kering.',\n",
       "  '### Instruction:\\nApa bahan utama membuat rendang sapi?\\n\\n### Response:\\nBahan utamanya adalah daging sapi, santan kelapa tua, dan campuran bumbu halus serta rempah daun.',\n",
       "  '### Instruction:\\nBagian daging sapi apa yang terbaik untuk rendang?\\n\\n### Response:\\nPaha belakang (knuckle) adalah yang terbaik karena teksturnya padat dan tidak mudah hancur.',\n",
       "  '### Instruction:\\nMengapa rendang dimasak sangat lama?\\n\\n### Response:\\nTujuannya agar santan terkaramelisasi menjadi minyak dan bumbu meresap sempurna ke dalam serat daging.',\n",
       "  '### Instruction:\\nApa perbedaan gulai, kalio, dan rendang?\\n\\n### Response:\\nGulai masih berkuah encer, kalio berkuah kental berminyak, dan rendang sudah kering serta berwarna gelap.']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be476b6",
   "metadata": {},
   "source": [
    "# Setup LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfaba134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\THINKPAD\\anaconda3\\envs\\rendang_gpt\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\"] \n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e787b4f6",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7d9be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mulai training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/176 1:30:21, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.146600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.673300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.348200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.593200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.336300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.964600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.924100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.924900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.927100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.904100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selesai! Model tersimpan di folder 'gpt2-rendang-final'\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-rendang\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=8,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    use_cpu=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Mulai training...\")\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./gpt2-rendang-final\")\n",
    "print(\"Selesai! Model tersimpan di folder 'gpt2-rendang-final'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff461321",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d129c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sedang memuat model hasil training...\n",
      "Sedang menghitung Perplexity...\n",
      "✅ Perplexity Score: 24.92\n",
      "Catatan: Semakin rendah skor (mendekati 1), semakin baik modelnya.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model_path = \"flax-community/gpt2-small-indonesian\"\n",
    "adapter_path = \"./gpt2-rendang-final\"\n",
    "\n",
    "print(\"Sedang memuat model hasil training...\")\n",
    "model_eval = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
    "model_eval = PeftModel.from_pretrained(model_eval, adapter_path)\n",
    "\n",
    "model_eval.to(device)\n",
    "model_eval.eval()\n",
    "\n",
    "tokenizer_eval = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer_eval.pad_token = tokenizer_eval.eos_token\n",
    "\n",
    "def calculate_perplexity(text_list, model, tokenizer):\n",
    "    encodings = tokenizer(\"\\n\\n\".join(text_list), return_tensors=\"pt\")\n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in range(0, seq_len, stride):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc \n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss * trg_len\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "    return ppl.item()\n",
    "\n",
    "texts_to_eval = dataset[\"text\"][:50]\n",
    "\n",
    "print(\"Sedang menghitung Perplexity...\")\n",
    "ppl_score = calculate_perplexity(texts_to_eval, model_eval, tokenizer_eval)\n",
    "\n",
    "print(f\"✅ Perplexity Score: {ppl_score:.2f}\")\n",
    "print(\"Catatan: Semakin rendah skor (mendekati 1), semakin baik modelnya.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0317c178",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db7e7daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat model untuk tes...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1. Setup Model (Sama seperti evaluasi)\n",
    "base_model_path = \"flax-community/gpt2-small-indonesian\"\n",
    "adapter_path = \"./gpt2-rendang-final\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Memuat model untuk tes...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_resep(pertanyaan):\n",
    "    # Format prompt HARUS SAMA PERSIS dengan saat training\n",
    "    prompt = f\"### Instruction:\\n{pertanyaan}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate jawaban\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,      # Batasi panjang jawaban biar gak ngelantur\n",
    "            do_sample=True,          # Supaya jawaban bervariasi\n",
    "            temperature=0.4,         # Kreativitas (0.1 kaku, 1.0 liar)\n",
    "            top_k=50,                # Ambil 50 kata terbaik\n",
    "            top_p=0.95,              # Ambil probabilitas kumulatif 95%\n",
    "            repetition_penalty=1.2,  # Cegah pengulangan kata\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode hasil (ubah angka jadi teks)\n",
    "    hasil_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Ambil bagian Response saja\n",
    "    if \"### Response:\" in hasil_text:\n",
    "        jawaban = hasil_text.split(\"### Response:\\n\")[1]\n",
    "    else:\n",
    "        jawaban = hasil_text\n",
    "        \n",
    "    return jawaban.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9aa27af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Pertanyaan: Apakah membuat rendang memerlukan bawang?\n",
      "Model sedang berpikir...\n",
      "------------------------------\n",
      "Jawaban Model:\n",
      "Untuk memasak rendang, kita perlu menggunakan bawang putih. Bawang putih memiliki kandungan sulfur yang tinggi dan bisa membantu proses pematangan bumbu. Selain itu, bawang putih juga mengandung antioksidan yang cukup tinggi.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 30)\n",
    "pertanyaan_kamu = \"Apakah membuat rendang memerlukan bawang?\"\n",
    "print(f\"Pertanyaan: {pertanyaan_kamu}\")\n",
    "print(\"Model sedang berpikir...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "jawaban_model = generate_resep(pertanyaan_kamu)\n",
    "print(f\"Jawaban Model:\\n{jawaban_model}\")\n",
    "print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rendang_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
